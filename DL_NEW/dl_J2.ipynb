{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b794408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.7009 - loss: 0.5275 - val_accuracy: 0.8479 - val_loss: 0.3414\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9563 - loss: 0.1225 - val_accuracy: 0.8143 - val_loss: 0.5136\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.9964 - loss: 0.0163 - val_accuracy: 0.8224 - val_loss: 0.6768\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 0.8278 - val_loss: 0.7534\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.9798e-04 - val_accuracy: 0.8285 - val_loss: 0.7901\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 1.0764e-04 - val_accuracy: 0.8295 - val_loss: 0.8195\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 5.6123e-05 - val_accuracy: 0.8302 - val_loss: 0.8430\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.2487e-05 - val_accuracy: 0.8305 - val_loss: 0.8666\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 2.1115e-05 - val_accuracy: 0.8312 - val_loss: 0.8896\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.3785e-05 - val_accuracy: 0.8316 - val_loss: 0.9134\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8286 - loss: 0.9278\n",
      "Test Accuracy: 83.16%\n",
      "Enter a movie review: wow what a great movie\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "Classification: Negative review\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load IMDB dataset with top 10,000 words\n",
    "max_words = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "# Pad sequences to a fixed length (e.g., 100 words)\n",
    "max_len = 100\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "# Define the DNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 64))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "batch_size = 32\n",
    "epochs = 10  # Increase the number of epochs\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the Model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Function to classify user input as positive or negative review\n",
    "def classify_review(input_text):\n",
    "    # Tokenize and pad user input\n",
    "    word_index = imdb.get_word_index()\n",
    "    input_sequence = [word_index[word] if word in word_index and word_index[word] < max_words else 0 for word in input_text.split()]\n",
    "    input_sequence = pad_sequences([input_sequence], maxlen=max_len)\n",
    "    \n",
    "    # Predict sentiment\n",
    "    prediction = model.predict(input_sequence)\n",
    "    if prediction[0][0] >= 0.5:\n",
    "        return \"Positive review\"\n",
    "    else:\n",
    "        return \"Negative review\"\n",
    "\n",
    "# Test the classification function with user input\n",
    "user_input = input(\"Enter a movie review: \")\n",
    "classification = classify_review(user_input)\n",
    "print(\"Classification:\", classification)\n",
    "'''Importing Libraries:\n",
    "numpy is a library for numerical operations in Python.\n",
    "tensorflow.keras is part of TensorFlow, a popular deep learning framework. Keras is a high-level API that allows for easy construction, training, and evaluation of neural networks.\n",
    "Loading IMDb Dataset:\n",
    "The IMDb dataset is a collection of movie reviews labeled as positive or negative sentiments.\n",
    "num_words=max_words restricts the dataset to the top 10,000 most frequent words, reducing complexity while retaining important information.\n",
    "Preprocessing Data:\n",
    "pad_sequences is used to ensure all sequences (movie reviews) have the same length.\n",
    "This is crucial for training neural networks because they typically expect fixed-size inputs.\n",
    "Defining the Neural Network Model:\n",
    "Sequential() creates a linear stack of layers for the model.\n",
    "Embedding layer converts word indices into dense vectors of fixed size (64 in this case), which helps in capturing semantic relationships between words.\n",
    "Flatten layer reshapes the 2D output of the embedding layer into a 1D vector for further processing.\n",
    "Dense layers are fully connected layers with a specified number of units and activation functions.\n",
    "The final Dense layer with sigmoid activation performs binary classification, predicting whether a review is positive or negative.\n",
    "Compiling and Training the Model:\n",
    "model.compile configures the learning process with an optimizer (adam), loss function (binary_crossentropy for binary classification), and metrics to monitor (accuracy in this case).\n",
    "model.fit trains the model using the training data (x_train and y_train) for a specified number of epochs (10 in this case) and a batch size of 32. Validation data (x_test and y_test) are used to evaluate the model's performance during training.\n",
    "Evaluating the Model:\n",
    "model.evaluate computes the loss and metrics (accuracy in this case) of the model on the test data (x_test and y_test).\n",
    "The results are printed to assess how well the model generalizes to unseen data.\n",
    "Classification Function:\n",
    "classify_review is a function that takes user input (a movie review) and predicts its sentiment (positive or negative).\n",
    "It tokenizes the input text using the IMDb dataset's word index, converts it into sequences, pads it to a fixed length, and then uses the trained model to predict the sentiment.\n",
    "Testing the Classification Function:\n",
    "The code prompts the user to enter a movie review.\n",
    "It calls the classify_review function to classify the user's input as a positive or negative review based on the trained model.\n",
    "The result is printed, indicating whether the input review is classified as positive or negative.\n",
    "In summary, this code demonstrates the entire pipeline of building a sentiment analysis model using a neural network, training it on IMDb movie review data, and using it to classify user-provided text into positive or negative sentiments. It showcases fundamental concepts in natural language processing (NLP) and deep learning for text classification tasks.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bbb4043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a movie review: The soundtrack perfectly complements the mood of each scene, enhancing the overall impact.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Classification: Positive review\n"
     ]
    }
   ],
   "source": [
    "# Test the classification function with user input\n",
    "user_input = input(\"Enter a movie review: \")\n",
    "classification = classify_review(user_input)\n",
    "print(\"Classification:\", classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b72a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecc4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
